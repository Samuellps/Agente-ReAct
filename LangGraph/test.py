import extractor
from langchain_openai import ChatOpenAI
from langgraph.graph import MessagesState
from langchain_core.messages import HumanMessage, SystemMessage
from langgraph.graph import START, StateGraph
from langgraph.prebuilt import tools_condition
from langgraph.prebuilt import ToolNode
from dotenv import load_dotenv
import os

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

tools = [extractor.search_catalog]
llm = ChatOpenAI(model="gpt-4o", api_key=api_key)
llm_with_tools = llm.bind_tools(tools)

# System message
sys_msg = SystemMessage(content="""
Voc√™ √© um assistente inteligente que ajuda o usu√°rio a encontrar informa√ß√µes espec√≠ficas em um banco de dados de texto.  

### **üìå Instru√ß√µes importantes:**  
- **Sempre tente usar `search_catalog` antes de pedir mais detalhes ao usu√°rio.**  
- Se a pergunta puder ser respondida diretamente com seu conhecimento, responda normalmente.  
- Caso contr√°rio, **sempre** consulte `search_catalog` para buscar informa√ß√µes relevantes.  
- **Apenas se `search_catalog` n√£o retornar resultados √∫teis, informe o usu√°rio de maneira educada.**  
- Explique ao usu√°rio quando estiver buscando informa√ß√µes no banco de dados.  
- **Integre os resultados da pesquisa de forma coesa e contextualizada na resposta final.** 
- Sempre considere que a informa√ß√£o pedida est√° contida no banco de dados e que ela pode ser acessada (n√£o h√° informa√ß√µes confidenciais) 

### **üìå Sobre o banco de dados:**  
O banco cont√©m diversos tipos de arquivos de texto, incluindo artigos e cat√°logos de produtos.  

### **‚úÖ Exemplo de fluxo correto:**  
Usu√°rio: "Summarize the article's methodology."  
Voc√™: `"Vou consultar o banco de dados para obter informa√ß√µes sobre a metodologia do artigo."`  
üìå Usa `search_catalog("article methodology")`  
üìå Se encontrar resultados, integra √† resposta final.  
üìå Se n√£o encontrar, ent√£o informa o usu√°rio.  

Seja claro, informativo e sempre considere o contexto da conversa.
""")

# Node
def assistant(state: MessagesState):
   return {"messages": [llm_with_tools.invoke([sys_msg] + state["messages"])]}


# Criando o Grafo do LangGraph
builder = StateGraph(MessagesState)

# Adicionar n√≥s ao LangGraph
builder.add_node("assistant", assistant)
builder.add_node("tools", ToolNode((tools)))  # Adiciona a Tool de busca

# Conectar os n√≥s
builder.add_edge(START, "assistant")
builder.add_conditional_edges(
    "assistant",
    tools_condition,
)
builder.add_edge("tools", "assistant")  # Continua a conversa ap√≥s a busca

# Compilar o grafo
graph = builder.compile()

# **Executando o Agente**
query = "Preciso de um protetor solar fator 50. Tem no catalogo? Se n√£o tiver o fator 50, tem outro mais proximo?"
resposta = graph.invoke({"messages": [HumanMessage(content=query)]})

for m in resposta['messages']:
    m.pretty_print()